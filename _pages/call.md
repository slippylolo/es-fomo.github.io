---
layout: about
title: üìù call for papers
permalink: /call/
subtitle: <i>Workshop at the International Conference on Machine Learning (ICML) 2023.</i>
nav: true
---

## call for papers
*(submission link will be shared as soon as logistics permit!)*

We welcome submissions addressing **emerging research questions and challenges associated with foundation model training and inference** (see below for a list of relevant topics). Notably, we encourage submissions concerning the entire spectrum of foundation models: from BERT-sized Transformers, to large models with 100B+ parameters. 

**We specifically welcome contributions which may be more engineering/code-focused than what is commonly accepted at machine learning conferences, and which feature open codebases.** Implementation details, performance benchmarks, and other nitty gritty details are welcome at this workshop.


Accepted works will be presented as posters during the workshop, and selected works will be featured as contributed talks.

### üìÜ important dates

* Submission deadline: **`TBD`**;
* Acceptance notification: **`TBD`**;
* Camera-ready deadline: **`TBD`**;

### ‚ùì relevant topics 
Topics include but are not limited to:
* **Training and inference systems**:
  * Training and inference of large models;
  * Training on cloud/heterogeneous infrastructure;
  * Training and inference in bandwith/compute/memory/energy-constrained settings;
  * Decentralized/collaborative training and inference;
* **Algorithms for improved training and inference efficiency**:
  * Large batch training and optimization;
  * Algorithms for communication-efficient training. 
* **Systems for foundation models**: 
  * Programming languages and compilers for efficient training and inference; 
  * Low-precision training and inference;
  * Benchmarks for large models training and inference.

### ü•∏ additional details

* **Formatting:** all submissions must be in PDF format, following the [ICML template](https://media.icml.cc/Conferences/ICML2023/Styles/icml2023.zip). We recommend submissions to be concise, up to four pages of main content, with unlimited additional pages for references and supplementary--although this will not be enforced as a hard limit. Supporting code may be provided either as a supplementary .zip or as a link to an anonymized repository.
* **Reviewing:** reviewing will be double-blind, so authors should take care to adequately anonymize their submission, especially regarding supplementary code. Reviewers will be instructed to focus on correctness and relevance to the workshop.
* **Non-archival:** this workshop is non-archival and will not result in proceedings; workshop submissions can be submitted to other venues.